{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Healthy Hungry -- San Francisco ([H3](http://en.wikipedia.org/wiki/Marine_chronometer))\n",
    "\n",
    "In this analysis, I will walk you through my general data science [process](http://zipfianacademy.com/data/data-science-process.png) by analyzing the inspections of San Francisco restaurants using publicly available [data](http://www.sfdph.org/dph/EH/Food/score/default.asp) from the Department of Public health.  We will explore this data to map the cleanliness of the city, and get a better perspective on the relative meaning of these scores by looking at statistics of the data.   Throughout this notebook, I show how to use a spectrum of powerful tools for data science (from UNIX shell to pandas and matplotlib) and provide some tips and data tricks.  If you would just like to see what insights we garnered, read the associated blog [post]() or simply jump to the [bottom](#results) of this notebook.  This notebook can be downloaded (with associated data) from its [repo](https://github.com/Jay-Oh-eN/happy-healthy-hungry).\n",
    "\n",
    "__Our approach is documented here to encourage anyone (and everyone) to repeat our analyses and to provide complete transparency into our results. And because we love the [scientific method](https://en.wikipedia.org/wiki/Scientific_method)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's More!\n",
    "\n",
    "This is one of many projects aimed at democratizing data and letting it roam free (free-range data?).  Stay tuned to our [feeds](http://twitter.com/zipfianacademy) or [blog](http://blog.zipfianacademy.com) if this is the kind of thing that excites you!\n",
    "\n",
    "If you want to learn more or dive deeper into any of these subjects, we are always happy to discuss (and can talk for days). And if you just can't get enough of this stuff (and want a completely immersive environment), you can [apply](http://zipfiancollective.wufoo.com/forms/m7x3z9/) for our intensive data science bootcamp starting September 16th.\n",
    "\n",
    "We would love to hear from all of you! Please reach out with any suggestions, feedback, or to just say hi: <a href=\"mailto:jonathan@zipfianacademy.com\">jonathan@zipfianacademy.com</a>\n",
    "\n",
    "__If you would like to get involved with our school please [contact](http://zipfianacademy.com/getinvolved/) us!  We are always looking for guest speakers/instructors, novel datasets, and companies to partner with either for internships/externships or for our hiring day__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pylab to provide scientific Python libraries (NumPy, SciPy, Matplotlib)\n",
    "%pylab --no-import-all\n",
    "#import pylab as pl\n",
    "# import the Image display module\n",
    "from IPython.display import Image\n",
    "\n",
    "# inline allows us to embed matplotlib figures directly into the IPython notebook\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/animate.gif', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Problem___\n",
    "\n",
    "The first step of the __Process__ is to define the problem we want to address.  To do so let us review what we have set out to accomplish and begin exploring questions we want answered.\n",
    "\n",
    "> ### How clean are SF restaurants?\n",
    "\n",
    "It is often best to arrive at a simple yet illuminating question to give you direction.  Of course there are a number of sub-questions we may have that relate to our over arching problem, we can address these when we determine our goals for the analysis.\n",
    "\n",
    "#### Let us review the important points to keep in mind when defining our problem:\n",
    "    \n",
    "* The question can be ___qualitative___, but the approach, must be ___quantifiable___ \n",
    "* What am I ___looking___ for? \n",
    "* What do I want to ___learn___?\n",
    "* Alright to be ___exploratory___ (and the best analysis often are)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/goal.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"goals\"></a>\n",
    "## ___Determine Goal___\n",
    "\n",
    "Now that we have a problem we hope to solve, let us begin to quantify our analysis.  Since our _Problem Statement_ is often qualitative and broad, we can ask further questions to better define what we hope to achieve.\n",
    "\n",
    "> How does an individual restaurants' score compare to the whole/aggregate of SF?\n",
    "\n",
    "> Are SF's inspections better or worse than other cities?\n",
    "\n",
    "> If a restaurant has not yet been inspected, can we approximate/predict what score it will receive? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things to note about our goals and approach:\n",
    "\n",
    "* Determine what defines __success__, and to what degree.\n",
    "* Brainstorm __metrics__ to visualize and/or calculate.\n",
    "* Ask __questions__ that have (or can have) a definitive answer.\n",
    "* Be careful what you wish for, be aware of possible __correlations__, and take caution with how you [measure](http://en.wikipedia.org/wiki/Observer-expectancy_effect) it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/explore.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Explore Data___\n",
    "\n",
    "#### To recap where we are in our analysis:\n",
    "    \n",
    "* We have determined what we want to learn -- ___How clean are SF restaurants?___\n",
    "* We explored quantifiable metrics to collect -- ___Individual scores, summary statistics about distribution of scores, other cities' inspection data to compare___ \n",
    "\n",
    "The ___Explore___ stage of the analysis is where we will most likely spend most of our [time](http://strataconf.com/stratany2012/public/schedule/detail/27495).  Now comes the fun part (in my opinion)!  At this stage we will use a variety of tools (the documentation of each linked to inline) to figure out where and how to obtain data, what it looks like once we have it, and how to use it to answer of questions to achieve our <a href=\"#goals\">goals</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire\n",
    "\n",
    "Luckily, San Francisco has much of its public government data freely [accessible](https://data.sfgov.org/) online.   There are also great [initiatives](http://www.yelp.com/healthscores) by SF [companies](http://officialblog.yelp.com/2013/01/introducing-lives.html) collaborating with [non-profits](http://codeforamerica.org/) and [government](http://sfgov.org/) to develop open data [standards](http://foodinspectiondata.us/).  Such standardization allows for much more transparency, leading ultimately to a more engaged citizenry.\n",
    "\n",
    "The relevant [data](http://www.sfdph.org/dph/EH/Food/score/default.asp) has been downloaded for your convenience and can be found in the [repo](https://github.com/Jay-Oh-eN/happy-healthy-hungry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine\n",
    "\n",
    "_If you are working with this IPython notebook, download the data files into the same directory which you ran the_ `ipython notebook`  _command_\n",
    "\n",
    "Now that we have found the relevant data we can begin to peer inside to understand what we are working with.  I recommend starting with an iterative approach, using the quickest/easiest tools first and slowly build to more complicated analyes.  UNIX provides us with many powerful tools and can carry us quite far by itself. In our case the dataset came with [documentation](https://s3.amazonaws.com/piazza-resources/hgtp0qhpaps1d5/hhfjlqv3gii2l8/File_Specifications.pdf?AWSAccessKeyId=AKIAJKOQYKAYOBKKVTKQ&Expires=1370258028&Signature=ZsHGKBMNwbdv9Ptio3b6GYrhB08%3D) of its contents, but it still is essential to look at the raw data and compare it to the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us display a few lines of data to understand its format and fields\n",
    "\n",
    "# Any command prefixed with '!' instructs IPython to run a shell command\n",
    "# http://ipython.org/ipython-doc/rel-0.13.1/interactive/reference.html#system-shell-acces\n",
    "\n",
    "!head -n 5 data/SFBusinesses/businesses.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`head` is a UNIX command to print only the first few lines of a file (in this case 5).  This is very useful for exploring very large files (which happens a lot in data science) very quickly and easily.  You can read more about it [here](http://en.wikipedia.org/wiki/Head_(Unix)) or by consulting its manual pages on the command-line: `man head`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PROTIP]: IPython has built in tab completion for commands.  \n",
    "# Partially type a command or file name and hit tab.\n",
    "\n",
    "!head -n 5 data/SFBusinesses/inspections.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 data/SFBusinesses/violations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a little hard to read since the headers are much\n",
    "# shorter than the data.  Lets see if we can prettify it.\n",
    "\n",
    "!head -n 5 data/SFBusinesses/violations.csv | column -t -s ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`column` is used to format its input into multiple columns.  It also is useful for [formating](http://linux.about.com/library/cmd/blcmdl1_column.htm) columns already present in delimited data (CSV for example).  With this command we used UNIX [pipes](http://en.wikipedia.org/wiki/Pipeline_(Unix)), one of the most powerful and useful aspects of working in a terminal.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 data/SFBusinesses/ScoreLegend.csv | column -t -s ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___There are two different data directories, each of which has similar files.  Let's try to figure out the difference between the two, since the documentation on the data does not mention anything.___ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# We can use IPython cell 'magics' to run a cell in a subprocess. In this case we run\n",
    "# the entire cell in a bash process (notice no exclamation point before the shell command)\n",
    "\n",
    "head -n 5 data/SFFoodProgram_Complete_Data/businesses_plus.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IPython cell [magics](http://nbviewer.ipython.org/url/github.com/ipython/ipython/raw/master/examples/notebooks/Cell%20Magics.ipynb) and other tricks__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Can we somehow compare the columns?\n",
    "\n",
    "head -n 1 data/SFFoodProgram_Complete_Data/businesses_plus.csv | awk -F, '{ print NF }'\n",
    "head -n 1 data/SFBusinesses/businesses.csv | awk -F, '{ print NF }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We see here that in SFFoodProgram_Complete_Data/ the files seem to be augmented with additional data.  The file has almost twice as many fields present.__\n",
    "\n",
    "__We used [awk](http://en.wikipedia.org/wiki/AWK) to figure this out by passing in the header row from the file to a simple `awk` script to count the number of fields (NF).__\n",
    "\n",
    "__AWK is actually a programming language (in addition to a command line utility) and can by quite powerful if used correctly.  This is going to be one of the standard tools at our disposal as a data scientist to explore and manipulate data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the extra fields, is anything else different?\n",
    "\n",
    "!wc data/SFBusinesses/businesses.csv data/SFFoodProgram_Complete_Data/businesses_plus.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[wc](http://en.wikipedia.org/wiki/Wc_(Unix) is another standard UNIX utility that we will find ourself coming back to time and time again.  Here we compare the line counts (number of records) and sizes of the files.__\n",
    "\n",
    "__The first column is the number of newlines, or how many records are contained.  The second column is word count and the last the number of bytes__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting things to note from this very simple (yet illustrative) exploration of our data.  As we might have guessed, the files in the `SFFoodProgram_Complete_Data/` have more information added to the original `SFBusinesses/` files.  While the 'complete' data has more columns, there are actually fewer records (__6353 compared to 6384__) possibly due to the fact that it is harder to get the additional data for the businesses.  But while a few businesses might be missing (__~30__), there is almost twice as much data (__656KB compared to 1.2MB__) in the 'complete' files if we look at byte counts. \n",
    "\n",
    "We can endlessly explore and compare these files and contents, I encourage you to perform similar comparisons with the other extra files (`SFFoodProgram_Complete_Data/`) in the directories and dive deeper into each file itself.  For our examination of the data, I am happy with what we have accomplished.  Given these new insights, we have enough information to continue on with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "This is typically what people refer to as data 'munging' (or 'wrangling') and often is the most tedious process when working with messy data. Due to increasing awareness of the importance of data quality, the city of SF has been making great strides in more open and [accessible](http://www.datasf.org/) data. If you (the city of SF) know the [format](http://www.yelp.com/healthscores) you will need going into the data collection process (inspecting restaurants) you can hopefully avoid a lot of pain later in the analysis process. \n",
    "\n",
    "The preparation process of our analysis is not as long and cumbersome as it typically might be due to the high quality of the raw data.  Because of this, I will spare you much of the tedium of this step so we can focus on the more interesting aspects of the analysis.  If you want to see (and experience) the pain (all you masochists out there), we will get much deeper into data acquisition and scrubbing techniques in our data wrangling post of this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "Now that we know the structure of our data, we can start to begin examining it statistically to get a macrosopic look at its distribution.  This part of our tutorial will use much of the powerful built in functionality of [NumPy](http://www.numpy.org/), [SciPy](http://www.scipy.org/), [matplotlib](http://matplotlib.org/), and [pandas](http://pandas.pydata.org/).  If you want to get more experience with these, there are great [resources](http://fperez.org/py4science/starter_kit.html) and [tutorials](http://www.rexx.com/~dkuhlman/scipy_course_01.html) covering these libraries in much more [depth](http://scipy-lectures.github.io/) than I will here.  I highly recommend taking a look at these if this analysis interests you even in the least bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To perform some interesting statistical analyses, we first need to \"join\" our CSV files in order to associate businesses \n",
    "with their inspection scores. This data currently resides in SFBusinesses/businesses.csv and SFBusinesses/inspections.csv\n",
    "'''\n",
    "\n",
    "# import pandas library which provides an R like environment for python.\n",
    "# if you do not have it installed: sudo easy_install pandas.\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "# store relevant file paths in variables since we may use them frequently\n",
    "root_dir = 'data/SFBusinesses/'\n",
    "businesses = root_dir + 'businesses.csv'\n",
    "inspections = root_dir + 'inspections.csv'\n",
    "\n",
    "\n",
    "# load each file into a Pandas DataFrame, pandas automatically converts the first line into a header for the columns\n",
    "\n",
    "df_business = pd.read_csv(businesses)\n",
    "df_inspection = pd.read_csv(inspections)\n",
    "\n",
    "# inspect the first 10 rows of the DataFrame\n",
    "df_inspection.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we can 'join' DataFrames just as we would database tables\n",
    "pandas uses a left-outer join by default, meaning that all \n",
    "the records from the businesses will be present even if there\n",
    "is not a corresponding row in the inspections.\n",
    "'''\n",
    "\n",
    "# join the two DataFrames on the 'business_id' column\n",
    "big_table = df_business.merge(df_inspection, on='business_id')\n",
    "\n",
    "# the joined DataFrame columns: frame1 columns + frame2 columns\n",
    "# in our case it is the concatenation of df_business and df_inspection columns\n",
    "print 'Business:\\t' + str(df_business.columns) + '\\n'\n",
    "print 'Inspection:\\t' + str(df_inspection.columns) + '\\n'\n",
    "print 'Big Table:\\t' + str(big_table.columns)\n",
    "\n",
    "# allows for row and column indexing succinctly\n",
    "big_table.iloc[:10, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now that we have our joined data, we can start exploring it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first group our data by business so we can find its most recent score for the inspections\n",
    "\n",
    "grouped_business = big_table.groupby('business_id')\n",
    "\n",
    "# a funtion that takes a DataFrame and returns the row with the newest date\n",
    "def most_recent(df, column='date'):\n",
    "    return df.sort_index(by=column)[-1:]\n",
    "    \n",
    "# input to most_recent is the DataFrame of each group, in this case \n",
    "# all of the rows and columns for each business (grouped on business_id). \n",
    "most_recent_inspection_results = grouped_business.apply(most_recent)\n",
    " \n",
    "# We applied the most_recent function to extract the row\n",
    "# of the DataFrame with the most recent inspection.\n",
    "most_recent_inspection_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records without lat/long for mapping\n",
    "r = most_recent_inspection_results\n",
    "\n",
    "zero_filtered = r[(r['latitude'] != 0) & (r['latitude'] != 0)]\n",
    "\n",
    "filtered = zero_filtered.dropna(subset=['latitude', 'longitude'])[['business_id','name', 'address', 'Score', 'date', 'latitude', 'longitude']]\n",
    "\n",
    "filtered.to_csv('geolocated_rest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://inundata.org/R_talks/meetup/images/splitapply.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split-Apply-Combine\n",
    "A visual representation of how group-by, aggregate, and apply semantics work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can bin the restaurants by scores to understand the distribution of inspections better.  Here we create a histogram to understand the distribution of scores better__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "# create a matplotlib figure with size [15,7]\n",
    "figure(figsize=[15,7])\n",
    "\n",
    "# pandas built-in histogram function automatically distributes and counts bin values \n",
    "h = most_recent_inspection_results['Score'].hist(bins=100)\n",
    "\n",
    "# create x-axis ticks of even numbers 0-100\n",
    "xticks(np.arange(40, 100, 2))\n",
    "\n",
    "# add a title to the current figure, our histogram\n",
    "h.set_title(\"Histogram of Inspection Scores\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a basic idea of the distribution, let us look at some more interesting statistics\n",
    "scores =  most_recent_inspection_results['Score']\n",
    "mean = scores.mean()\n",
    "median = scores.median()\n",
    "\n",
    "# compute descriptive summary statistics of the inspection scores\n",
    "summary = scores.describe()\n",
    "\n",
    "mode = sp.stats.mode(scores)\n",
    "skew = scores.skew()\n",
    "\n",
    "# compute quantiles\n",
    "ninety = scores.quantile(0.9)\n",
    "eighty = scores.quantile(0.8)\n",
    "seventy = scores.quantile(0.7)\n",
    "sixty = scores.quantile(0.6)\n",
    "\n",
    "print \"Skew: \" + str(skew)\n",
    "print \"90%: \" + str(ninety)\n",
    "print \"80%: \" + str(eighty)\n",
    "print \"70%: \" + str(seventy)\n",
    "print \"60%: \" + str(sixty)\n",
    "print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/solutions.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Propose Solutions___\n",
    "\n",
    "Since we have explored our data and have a better idea of its nature, we can begin to devise a plan to answer our questions. This is usually the most iterative part of the entire process: as we learn more about our data we modify our approach, and as modify our solutions we must re-examine our data.\n",
    "\n",
    "#### Goals:\n",
    "> How does an individual restaurants' score compare to the whole/aggregate of SF?\n",
    "\n",
    "> Are SF's inspections better or worse than other cities?\n",
    "\n",
    "> If a restaurant has not yet been inspected, can we approximate/predict what score it will receive? \n",
    "\n",
    "#### Solutions:\n",
    "> Collect summary statistics (mean, median, standard deviation, etc.) about distribution of scores.\n",
    "\n",
    "> Acquire data on inspection scores for other cities, compare distribution of cities.\n",
    "\n",
    "> Perform a linear regression on historic data on past inspections combined with scores from other 'similar' restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things to note about formulating solutions:\n",
    "\n",
    "* Ask, how can I address the problem?\n",
    "* In what ways can I use the data to achieve my goals?\n",
    "* The simplest solution is often best (and the one you should try first)\n",
    "* Quantize the metrics needed for your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/metrics.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Collect Metrics___\n",
    "\n",
    "This is the step where derivative values are often calculated, including __summary statistics__, __transformations__ on the data, and __correlations__.  There also is a bit of traditional __data mining__ involved as most machine learning occurs in the solutions and metrics stages (in our formulation).  We could even go so far as to say that the results of predictive models are simply additional metrics: the __probability__ of defaulting on a loan, the __cluster__ a new product belongs in, or the __score__ of a restaurant that hasn't been inspected yet.\n",
    "    \n",
    "___The purpose of this part of the process is to calculate the information you need to begin evaluating and testing you solutions and hypotheses.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that in the Score Legend, each numeric score corresponds to a more qualitative description\n",
    "!head -n 5 data/SFBusinesses/ScoreLegend.csv | column -t -s ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "let us compute a histogram of these descriptions as well\n",
    "'''\n",
    "\n",
    "# first we need to discretize the numerical values, this can be \n",
    "# thought of as converting a continuous variable into a categorical one.\n",
    "descriptions = ['Poor', 'Needs Improvement', 'Adequate', 'Good']\n",
    "bins = [-1, 70, 85, 90, 100]\n",
    "\n",
    "# copy the scores from our grouped DataFrame, DataFrames manipulate\n",
    "# in place if we do not explicitly copy them.\n",
    "scores = most_recent_inspection_results['Score'].copy()\n",
    "score_transform = most_recent_inspection_results.copy()\n",
    "\n",
    "# built-in pandas funcion which assigns each data point in \n",
    "# 'scores' to an interval in 'bins' with labels of 'descriptions'\n",
    "discretized_scores = pd.cut(scores, bins ,labels=descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform the original DataFrame's \"Score\" column with the new descriptions\n",
    "score_transform['Score'] = discretized_scores\n",
    "\n",
    "score_transform[['name', 'date','Score']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__By quantizing the scores of the restaurant inspections, we can get a better qualitative insight into the ratings.  Let us compare this new distribution of quantized scores to the raw numeric values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/evaluate.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Evaluate___\n",
    "\n",
    "With the metrics we need properly calculated, it is time to draw some conclusions from our analyses.  We need to evaluate whether the result we have arrived at:\n",
    "    \n",
    "* Answers our original question to an acceptable level of confidence.\n",
    "* Has allowed us to achieve our goals? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot a histogram of the discretized scores\n",
    "'''\n",
    "\n",
    "# create a figure with 2 subplots\n",
    "fig = figure(figsize=[30,7])\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# count each occurance of descriptions in the 'Score' column,\n",
    "# and reverse this result so 'Poor' is left most and 'Good' right most\n",
    "counts = score_transform['Score'].value_counts()[::-1]\n",
    "plt = counts.plot(kind='bar', ax=ax2)\n",
    "\n",
    "# decorate the plot and axis with text\n",
    "ax2.set_title(\"Restaurant Inspections (%i total)\" % sum(counts))\n",
    "ax2.set_ylabel(\"Counts\")\n",
    "ax2.set_xlabel(\"Description\")\n",
    "\n",
    "# let us add some labels to each bar\n",
    "for x, y in enumerate(counts):\n",
    "    plt.text(x + 0.5, y + 200, '%.f' % y, ha='left', va= 'top')\n",
    "    \n",
    "# plot the original raw scores (same grapoh as earlier)\n",
    "most_recent_inspection_results['Score'].hist(bins=100, ax= ax1)\n",
    "# create x-axis ticks of even numbers 0-100\n",
    "ax1.set_xticks(np.arange(40, 100, 2))\n",
    "\n",
    "# add a title to the current figure, our histogram\n",
    "ax1.set_title(\"Histogram of Inspection Scores\")\n",
    "ax1.set_ylabel(\"Counts\")\n",
    "ax1.set_xlabel(\"Score\")\n",
    "\n",
    "savefig('histograms.png', bbox_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a majority of restaurants are __adequate__ or __good__, according to the quantiles only __25%__ have scores less than __88__.  While the histogram of the numeric scores gives us a more granular look at the data, it can be quite difficult to derive value from it.  Is an __86__ a filthy/unhealthy restaurant or did it simply forget a few nuanced inspection rules?  The Score Legend provides us a mapping from a raw score to a meaningful value, similar to the scaling of standardized test raw scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not satisfied with our evaluation, we need to iterate on our approach:\n",
    "    \n",
    "* Do I need more/better data?\n",
    "* Do I need to try a different proposed solution?\n",
    "* Do I need to calculate different metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matplotlib figure with size [15,7]\n",
    "figure(figsize=[15,7])\n",
    "\n",
    "# pandas built-in histogram function automatically distributes and counts bin values \n",
    "h = most_recent_inspection_results['Score'].hist(bins=100)\n",
    "\n",
    "# summary statistics vertical lines\n",
    "axvline(x=mean,color='red',ls='solid', lw=\"3\", label=\"mean\")\n",
    "axvline(x=median,color='green',ls='solid', lw=\"3\", label=\"median\")\n",
    "axvline(x=mode[0][0],color='orange',ls='solid', lw=\"3\", label=\"mode\")\n",
    "\n",
    "# 25th quantile\n",
    "axvline(x=summary['25%'],color='maroon',ls='dashed', lw=\"3\", label=\"25th\")\n",
    "axvspan(40, summary['25%'], facecolor=\"maroon\", alpha=0.3)\n",
    "\n",
    "# 75th quantile\n",
    "axvline(x=summary['75%'],color='black',ls='dashed', lw=\"3\", label=\"75th\")\n",
    "axvspan(40, summary['75%'], facecolor=\"yellow\", alpha=0.3 )\n",
    "\n",
    "# create x-axis ticks of even numbers 0-100\n",
    "xticks(np.arange(40, 104, 2))\n",
    "\n",
    "# add legend to graph\n",
    "legend(loc=2)\n",
    "\n",
    "# add a title to the current figure, our histogram\n",
    "h.set_title(\"Histogram of Inspection Scores\")\n",
    "\n",
    "savefig('quantiles.png', bbox_inches=0, transparent=True)\n",
    "\n",
    "print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Iterate___\n",
    "\n",
    "Now that we have a general idea of the distribution of these scores, let us see if we can find any correlation between score ranges and health violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import collections as c\n",
    "import pprint as pp\n",
    "\n",
    "# first let us form a 'big table' by joining the violations to the most recent inspection scores\n",
    "file=\"data/SFFoodProgram_Complete_Data/violations_plus.csv\"\n",
    "\n",
    "df_violations = pd.read_csv(file)\n",
    "\n",
    "violation_table = most_recent_inspection_results.merge(df_violations, on=['business_id','date'])\n",
    "violation_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how the violations are distributed\n",
    "figure(figsize=[18,7])\n",
    "\n",
    "violation_hist = violation_table['description'].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see what violations a restaurant can have and still get a perfect score\n",
    "\n",
    "figure(figsize=[18,7])\n",
    "\n",
    "perfect_scores = violation_table[violation_table['Score'] == 100]\n",
    "\n",
    "violation_hist = perfect_scores['description'].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "perfect_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_table['description'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm, apparently high risk vermin infestations are minor violations\n",
    "# If that is minor, what is a severe violation\n",
    "\n",
    "df_violations['ViolationSeverity'].drop_duplicates()\n",
    "\n",
    "# well aparently there are only minor violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us bin health violations using the cities quantizations\n",
    "\n",
    "descriptions = ['Poor', 'Needs Improvement', 'Adequate', 'Good']\n",
    "bins = [-1, 70, 85, 90, 100]\n",
    "\n",
    "# copy the scores from our grouped DataFrame, DataFrames manipulate\n",
    "# in place if we do not explicitly copy them.\n",
    "scores = violation_table['Score'].copy()\n",
    "violation_transform = violation_table.copy()\n",
    "\n",
    "# built-in pandas funcion which assigns each data point in \n",
    "# 'scores' to an interval in 'bins' with labels of 'descriptions'\n",
    "discretized_scores = pd.cut(scores, bins ,labels=descriptions)\n",
    "violation_transform[\"Scores\"] = discretized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = violation_transform.groupby('Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us find the most common violations for each group\n",
    "\n",
    "# a funtion that takes a DataFrame and returns the top violations\n",
    "def common_offenses(df):\n",
    "    return pd.DataFrame(df['description'].value_counts(normalize=True) * 100).head(10)\n",
    "\n",
    "top_offenses = grouped.apply(common_offenses)\n",
    "top_offenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = figure(figsize=[18,7])\n",
    "colors = ['r', 'b', 'y', 'g']\n",
    "\n",
    "for name, group in grouped:\n",
    "    group['description'].value_counts().plot(kind=\"bar\", axes=f, alpha=0.5, color=colors.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://assets.zipfianacademy.com/data/data-science-workflow/communicate.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "\n",
    "## ___Communicate Results___\n",
    "\n",
    "An important part of visualization that is often overlooked is informational accessibility, or rather how interpretable are the results.  Interactivity can go a long way towards making the content of your visualization much more digestable by allowing the consumer to 'discover' the data at their own rate. \n",
    "\n",
    "Each restaurant is geographically binned using the D3.js [hexbin](https://github.com/d3/d3-plugins/tree/master/hexbin) plugin.  The color gradient of each hexagon reflects the median inspection score of the bin, and the radius of the hexagon is proportional to the number of restaurants in the bin.  Binning is first computed with a uniform hexagon radius over the map, and then the radius of each individual hexagon is adjusted for how many restaurants ended up in its bin. \n",
    "\n",
    "Large blue hexagons represent many high scoring restaurants and small red mean a few very poorly scoring restaurants.  The controls on the map allow users to adjust the radius (__Bin:__) of the hexagon for computing the binning as well as the range (__Score:__) of scores to show/use on the map.  The color of the __Bin:__ slider represents the median color of the __Score:__ range and its size represents the radius of the hexagons.  The colors of each of the __Score:__ sliders represent the threshold color for that score, i.e. if the range is 40 - 100 the left slider's color corresponds to a score of 40 and the right slider to a score of 100.  The colors for every score in-between are computed using a linear gradient.  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<iframe width='100%' height='600' frameborder='0' src='http://assets.zipfianacademy.com/maps/h3/'></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "Thank you to [Yelp]() and [Code for America]() for spearheading the initiative to make civic data more open.  And this would not have been possible without the city of SF and all the health inspectors, thank you for keeping us safe.\n",
    "\n",
    "#### This has been a production of [Zipfian Academy](http://zipfianacademy.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
